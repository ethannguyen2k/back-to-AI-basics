{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f2f346",
   "metadata": {},
   "source": [
    "# Understanding Backpropagation\n",
    "\n",
    "Backpropagation is one of the foundational algorithms in neural network training. Let me break this down for you:\n",
    "\n",
    "## What is Backpropagation?\n",
    "\n",
    "Backpropagation (or \"backprop\") is actually a specific algorithm - contrary to your understanding, it is not just a general term for a learning system. It's a method for efficiently calculating gradients in neural networks by applying the chain rule of calculus in a clever way.\n",
    "\n",
    "The key idea of backpropagation is to calculate how each parameter (weight and bias) in the network affects the final output error, and then adjust those parameters to reduce the error. It does this by propagating the error backward through the network, hence the name.\n",
    "\n",
    "## When is Backpropagation Used?\n",
    "\n",
    "Backpropagation is used during the training phase of neural networks. Specifically:\n",
    "\n",
    "1. During each training iteration, after a forward pass calculates the network's prediction\n",
    "2. After the loss function computes how far off the prediction is from the actual target\n",
    "3. Before the optimization algorithm (like gradient descent) updates the parameters\n",
    "\n",
    "Virtually all modern neural networks - from simple MLPs to complex architectures like CNNs, RNNs, and Transformers - rely on backpropagation for training.\n",
    "\n",
    "## Backpropagation Exercise\n",
    "\n",
    "Let's design a simple exercise to understand backpropagation. We'll work with a minimal neural network with:\n",
    "- 2 input neurons\n",
    "- 2 hidden neurons\n",
    "- 1 output neuron\n",
    "\n",
    "```txt\n",
    "Input layer (x):      [x₁, x₂]\n",
    "Hidden layer (h):     [h₁, h₂]  \n",
    "Output layer (y):     [y]\n",
    "\n",
    "Weights:\n",
    "- W₁: 2×2 matrix connecting input to hidden\n",
    "- W₂: 2×1 matrix connecting hidden to output\n",
    "\n",
    "Biases:\n",
    "- b₁: for hidden layer\n",
    "- b₂: for output layer\n",
    "\n",
    "Activation function: sigmoid σ(x) = 1/(1+e^(-x))\n",
    "Loss function: Mean Squared Error (MSE)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d05346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55724785 0.62010643]\n",
      "[0.67932855]\n",
      "[0.00042731]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input data\n",
    "X = np.array([0.5, 0.3])\n",
    "\n",
    "# Output labels\n",
    "t = np.array([.7])\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights for the hidden layer and output layer\n",
    "w1 = np.array([[0.2, 0.4], [0.1, 0.3]])\n",
    "w2 = np.array([[0.5], [0.6]])\n",
    "b1 = np.array([0.1, 0.2])\n",
    "b2 = np.array([0.1])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1):\n",
    "    # Forward pass\n",
    "    hidden_layer_input = np.dot(X, w1) + b1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    final_output = np.dot(hidden_layer_output, w2) + b2\n",
    "    y = sigmoid(final_output)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = np.square(y-t)\n",
    "    d_predicted_output = 2 * (y - t) * sigmoid_derivative(y)\n",
    "\n",
    "    error_hidden_layer = d_predicted_output.dot(w2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights\n",
    "    w2 += hidden_layer_output.reshape(-1, 1).dot(d_predicted_output.reshape(1, -1)) * lr\n",
    "    w1 += X.reshape(-1, 1).dot(d_hidden_layer.reshape(1, -1)) * lr\n",
    "    b2 += np.sum(d_predicted_output, axis=0) * lr\n",
    "    b1 += np.sum(d_hidden_layer, axis=0) * lr\n",
    "\n",
    "print(hidden_layer_output)\n",
    "print(y)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25095f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04134291]\n",
      "[[-0.00501868]\n",
      " [-0.0055848 ]]\n",
      "[[-0.00450314 -0.00540377]]\n",
      "[[-0.00055551 -0.00063649]\n",
      " [-0.00033331 -0.0003819 ]]\n"
     ]
    }
   ],
   "source": [
    "# Backward pass (manual calculation for gradients)\n",
    "# The gradient of E with respect to y\n",
    "grad_E_y = 2 * (y - t)\n",
    "# The gradient of E with respect to W₂\n",
    "grad_E_W2 = grad_E_y * sigmoid_derivative(y) * hidden_layer_output.reshape(-1, 1)\n",
    "# The gradient of E with respect to h\n",
    "grad_E_h = grad_E_y * sigmoid_derivative(y) * w2.T\n",
    "# The gradient of E with respect to W₁\n",
    "grad_E_W1 = grad_E_h * sigmoid_derivative(hidden_layer_output) * X.reshape(-1, 1)\n",
    "\n",
    "print(grad_E_y)\n",
    "print(grad_E_W2)\n",
    "print(grad_E_h)\n",
    "print(grad_E_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6861c9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20005613 0.40006431]\n",
      " [0.10003368 0.30003858]]\n",
      "[[0.50050706]\n",
      " [0.60056425]]\n",
      "[[0.10011357 0.20012976]]\n",
      "[0.10090993]\n"
     ]
    }
   ],
   "source": [
    "# Parameter updates (manual calculation)\n",
    "w1 = w1 - lr * grad_E_W1\n",
    "w2 = w2 - lr * grad_E_W2\n",
    "b1 = b1 - lr * grad_E_h * sigmoid_derivative(hidden_layer_output)\n",
    "b2 = b2 - lr * grad_E_y * sigmoid_derivative(y)\n",
    "\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
